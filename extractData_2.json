{"id": "46901857", "Abstract": "Multimedia production displays two faces: a multimedia product is the result of pro-gramming as well as of publishing. Constructing a multimedia environment for teaching acourse in the history of arts suggests that requirements elicitation has many facets and iscentral to success, hence has to be delt with in a particularly careful way, the more so sincewe wanted the art historians working in a speci\fc and custom tailored environment. Theproblem was technically resolved by constructing a dedicated markup language based onXML. We discuss the process of requirements elicitation that led to the markup languageand show how this is used as a cornerstone in the development of such an experimentalenvironment.Keywords and Phrases: Multimedia environment, XML, DTD, Altenberger DomMarkup Language, multimedia in teaching the history of arts, requirements elicitation,translation and interpretation.\u0003The work reported here has been partially funded by Universit\u007fatsverbund Multimedia Nordrhein-Westfalen1Towards Constructing a Flexible Multimedia Environment for Teaching", "1 Introduction": "Teaching the history of art does not yet fully capitalize on the possibilities o\u000bered by mul-timedia. This is surprising since the history of art is nourished by visual representations.On the other hand, scholarly ideas are presented quite similarly to teaching philosophy, soon second thought the lack of multimedia teaching material may be attributed to not (yet)having a sturdy bridge between ideas and pictures. Of course, teachers in art history havelong made use of visual aids, usually displaying slides or pictures. This kind of presenting thenecessary visual information is not fully satisfactory to either students or instructors:\u000f slides and pictures in books are quite static, some of the things to be presented aredynamic, hence change has to be related explicitely. This is achieved usually through awell chosen sequence of pictures, conveying dynamics, albeit only to a limited extent,\u000f textual and visual information is linked conventionally through narration; students \fndit di\u000ecult reestablishing links when rehearsing the lecture: pictures and explanations(or explications) are gone, taking notes is not easy, in particular when the lecture hallis dark,\u000f background information is sometime needed, but not alway at the \fngertips of instruc-tors or students when a topic is conventionally presented.A multimedia presentation links text and pictures lively, e\u000bectively and in a customizablemanner, easing the burden of establishing an mental connection between textual and pictorialrepresentation. It may o\u000ber an explanation for the evolution of phenomena, hence capturingdynamics, and it may provide ample background information through hyperlinks. This is ourvision.Certainly we do not want to leave the impression that the conventional mode of teaching isobsolete, bad or plainly unattractive. What we are pleading | and working | for is utilizingmultimedia as a device for complementing and embellishing the traditional approach.We decided entering a cooperation with the Chair for Architectural History at our university.This cooperation aims at the construction of a multimedia representation of the AltenbergCathedral. The cathedral is a well known and important Gothic church in the Rhineland. Itentertains features that are of interest to future architects as well as civil engineers. Thesestudents have to take a course in architectural history in which the cathedral features promi-nently.From the beginning we understood that both parties would bene\ft from this project in ratherspeci\fc ways. The art historians would obtain a multimedial complement to their textbooks,and they would construct a vehicle for teaching their students the in-depth work with the newelectronic tools. We as software engineers would help in constructing this vehicle, gaininginsight into an intellectually very attractive \feld together with obtaining a possibility oflearning about the architecture of multimedia systems. Most important, we \fnd it excitingto gain some experience in requirements elicitation in a \feld that has not seen too manycomputer scientists' footsteps.The present paper is intended to report on this joint project from the software engineer'sperspective. We want to relate where we encountered principal di\u000eculties, how we triedto overcome them, and what we learned from these experiences. In particular we draw thereader's attention to problems of requirements elicitation that emerge in a context where2Towards Constructing a Flexible Multimedia Environment for Teachingscholars and engineers cooperate. Finally we report on a solution for the construction of amultimedia environment, and we indicate where we endeavour future work.2 The PerspectiveIntroducing computer-based training into an art history course is by no means a trivial un-dertaking. Usually the infrastructure is not comparable to a setting in which engineeringcourses are taught, but the missing infrastructure may easily be overcome. But the reasonsfor the non-triviality lie deeper.Consider these observations we made when cooperating with the group from art history:\u000f traditionally there is enough pictorial material to be taught from, including graphicsused e.g. to analyze a given architectural situation. This material, however, is sometimesnot particularly suited for being used in a multimedia environment.This is so since it is organized according to principles which are introduced to servethe human user, but not to be applied with a computer; the informal organizationalschemes are di\u000ecult | if not impossible | to map to a maintainable database design.\u000f we found it di\u000ecult to talk to each other | the way art historians organize their teachingis understandably di\u000berent from the way computer scientists do, and vice versa.Hence requirement elicitation had to be preceded by a phase of learning of mutualunderstanding.\u000f the necessity of our doing preparatory technical work was not always immediate to ourart historians.Hence there was an incentive to have quite early something that could be presentedrather in preference to working on seemingly intangible speci\fcations.We resolved these problems in our cooperation by attempting to agree on the following ap-proach:1. the target platform is factored out, so that work can be done independent of a particularhardware or software platform,2. requirements are going to be \fxed and described in a sequence of scenarios, which inturn will be translated into a representable form,3. we intend to construct a development environment that should be user friendly enoughso that non-computer scientists will be able to work with it. The environment willeventually be process-based.3 Related WorkThe ZYX model [BK99] is used for semantic modeling of multimedia content. The ZYX mul-timedia document model has been developed in the context of a database-driven multimediainformation system. The model describes multimedia documents based on a tree. The nodes3Towards Constructing a Flexible Multimedia Environment for Teachingof the tree are presentation elements like videos, images and text or operator elements allow-ing temporal synchronization, de\fnition of interaction, spatial and audible layouting. Themodel o\u000bers support in structuring the presentation document and combining media objectsat layout design level and do not support a didactical design and instantiation phase withlogical elements, which should precede synchronization and layouting of media in the softwaredevelopment process of a multimedia teaching system (MMTS).The process model for the development of multimedia applications in [DEM+99] is basedon an analysis and a design phase using the programming model of an authoring system(Director) and on implementing with this speci\fc authoring system. First, an analysis modelof the application is constructed and a programming model of the authoring system is derived.Then the relation between these models is analyzed. An instance application model is mappedto an authoring system model using this relation. This approach employs an analysis anddesign phase trying to \fll the gap between the analysis model and implementation using anauthoring system. It was developed in parallel to our approach and is rather closely tiedto the programming model of an authoring system. It is abstract in the sense that it doesnot cater for a particular application, and it is quite speci\fc in the use of the programmingtools. In contrast to this model, we emphasize the role of domain analysis in the constructionof the language we use, and of supporting tools maintaining a platform independent way ofrepresentation.The RelationshipManagement Methodology (RMM) [ISB95], Hypertext Design Model (HDM)[GPS93] and Object-Oriented Hypermedia Design Model (OOHDM) [SR95] are used for hy-permedia or multimedia applications in those areas that display a high degree of domainstructure.Nanard et al. discuss in [FNN96] their SGML-based authoring system PageJockey and presentthe process between sketches, templates, and \fnal releases of multimedia applications realizedin di\u000berent SGML languages. The technical focus lies on the abstract models and the mappingbetween them, the methodical on the design process of visual design. They are aiming on anearly visual feedback of application derived automatically from sketches and templates.Morris and Finkelstein [MF92] are discussing a method for constructing a hypertext in theeld of history of art, later generalized as a design process integrating the visual designersin an explicit way [MF96, ]. It is not clear in how far their method could be supported bya development environment, because the transformation process of media formally describedin the model is not fully operational.4 The Formal Structure of Multimedia ApplicationsIn the methodologies [ISB95, GPS93, SR95] quoted above the structure of the presentationis derived from the structure of the application's domain or content. This is just one pointin a spectrum: the pronounced structuring of a domain induces a formal system structurethat can be handled with comparable ease. Figure 1 indicates that there are other situations,classi\fed against number of classes vs. number of objects drawn from each class. Generallyspeaking, many classes in a system arise whenever the application domain admits a modelwith a highly perceivable formal structure, many instances of a class are generated when ahigh degree of similarity has to be captured, see e.g. [Jac92, II.7] or [GHJV95, p. 107], inparticular the discussion leading to the Factory pattern. Now consider Figure 1. It illustrates4Towards Constructing a Flexible Multimedia Environment for Teachingtypical hypermedia and multimedia applications classi\fed in relation to the number of classesand the number of instances of each class represented by the two axes.Art HistoryLiterary WorkKioskApplicationNews Servicenumber of classesin the systemof each classnumber of instancesProduct CatalogFigure 1: The kinds of hypermedia and multimedia applicationsThe domain presented in a product catalog [SKT+97] is characterized by a large numberof product types (classes) and a large number of products for each product type (objects).Front-ends of databases are quite similar. The so called point of information (POI) as atypical application of kiosk systems has to present many di\u000berent \felds (i.e. the classes inthis domain) of an enterprise, a city, a railway station, a museum, etc., but may have justone instance for each \feld or at most a very small number of instances.On the other hand there are applications with a small number of classes in the domainrepresented. A News service presented by a television channel or a news site presented byan enterprise on the Internet is a type of application that is adequately represented by justone class. This kind of application is characterized by the restriction of constructing onlyinstances of this class.Our domain in this project is architectural history. This domain has a low formal structurewith a small number of classes and a small number of instances, often unique objects. Thecomplexity of the domain is spread out over a few classes and these classes will be verycomplex. So the usefulness of the mentioned methods for our domain (art history) is low,because these methods base on domain modelling with a supposed regular structure of thedomain.A small number of classes and a small number of instances in the system contains each kindof application presenting artistic work as content, e.g. literary work or art history. Thesedomains, although intrinsically highly structured, present hardly any handle of structuringthem formally according to our needs. Thus their presentation in a multimedia applicationdoes not depend on the formal content structure (for example, structuring novels into chaptersfor Electronic Books does usually no justice to the artistic quality). A structuring facilityfor presenting architectural history could e.g. be a presentation form like a Guided Tour.Although the construction of such a tour is not the intent of our project, we adopt some ofthe ideas, as will be seen later.5Towards Constructing a Flexible Multimedia Environment for Teaching5 Our Approach to the Development Process of a MultimediaTeaching SystemMultimedia applications are large and complex software systems, which should be developedusing software engineering methods. Existing authoring systems like Director, Toolbook orAuthorware support only the programming phase of the software process, similar to inte-grated environments like Visual C++, Delphi, etc. They do not support modeling during theanalysis and design phase. But modeling should precede implementation in the developmentof software systems.The development of a multimedia application is di\u000berent from the development of software.We want to develop a multimedia presentation (MMP) which turns out to be both a programand a document | this is an important observation for the development that follows. Thisexistence of di\u000berent views is acknowledged by the parties involved: the art historians perceivethe presentation as a document with a course structure and media content. The softwareengineers see the presentation as a program. By this duality of multimedia presentations asprograms and as documents the construction of a MMP necessarily intertwines methods fromsoftware construction and from publishing.Furthermore we had to develop our MMP as a multimedia teaching system (MMTS). For thiskind of application the presentation structure depends also heavily on the didactical conceptand is in this respect quite independent of the content and its structure. Therefore we needan approach supporting integration between software development, publishing and teaching.It is this approach that will be presented here.Since markup languages rely on a formal syntax and on the incorporation of narrative text,they are ideally suited for our project once they permit enough \rexibility for processingmultimedia documents. The class of languages suited to our needs is currently representedby the XML approach [BPSM98], XML instances (or languages) being markup languagessimilar to SGML instances, HyTime [NKN91] or HTML. These languages are all de\fned by adocument type description (DTD). Hence a particular instance of XML is essentially de\fnedby a DTD. We felt that the conversion of our requirements to a DTD would be the mostappropriate way of dealing with the restrictions imposed on us. Documents written in thisspeci\fcation language are converted into an executable program interpreted by an authoringsystem, the Macromedia Director.Thus, our approach is divided into two separate parts. The \frst part deals with the de\fnitionof a DTD suitable for our MMTS. The second part makes use of this DTD for writingdocuments and for building the MMTS itself.The DTD-De\fnition Step. This step is subdivided into two sub-steps. First, we establishan analysis phase aiming at the structure, the layout and the capture of the didactical conceptof the MMTS. This phase is mostly driven by the art historians consulted by the computerscientists to ensure the technical feasibility of their ideas. The second sub-step formalizesthe results of the analysis as an DTD which we call Altenberger Dom Markup Language(henceforth abbreviated by ADML; the Altenberg Cathedral is the only church we know ofthat has a programming language named after itself). This is the work of the computerscientists consulted by the art historians to ensure that the formalization is a valid one. Insummary, the \frst part requires and constitutes an extensive dialog between art historians6Towards Constructing a Flexible Multimedia Environment for Teachingand computer scientists resulting in layout, structure and didactic concepts as well as theircapture in a DTD.Splitting the Groups. The second part splits the two developer groups. The art historianswrite documents in ADML, formally instances of the ADML-DTD, create media objects andcombine them into scenes. This is the publishing step. The computer scientists realize thetechnical part of the MMTS and build a converter for the transformation of the ADMLdocuments into an executable multimedia presentation.The approach to the development process of a multimedia learning system in the AltenbergCathedral Project is discussed in the following sections and is sketched in Figure 2.in the languageDescriptionof the ideaComputer ScientistsConverterand VAM"}{"id": "80133310", "Abstract": "The studies based on auto-epistemic logic are pointed out as an advanced direction for development of artificialintelligence (AI). Artificial intelligence is taken as a system that imitates the solution of complicated problems byhuman during the course of life. The structure of symbols and operations, by which intellectual solution isperformed, as well as searching the strategic reference points for those solutions, which are caused by certainstructures of symbols and operations, \u2013 are considered among the main issues in analysis of AI and itsapplications. Expert systems are interpreted as a kind of intelligent systems; different ways to representknowledge (such as logical model, frame-based and production systems, semantic networks) are described withinthe framework of cognitive studies of AI. The presentation of knowledge is stated to be the methodology formodeling and formalization of conceptual knowledge in the field of engineering.\u00a9 2014. The Authors. Published by Elsevier Ltd.Selection and peer-review under responsibility of Tomsk Polytechnic UniversityKeywords: Artificial intelligence, knowledge, meta-knowledge, presentation of knowledge, knowledge processing, singularity."}{"id": "80135094", "Abstract": "The vision of the surrounding and people that are within eyeshot influences the human well-being and safety. Therationale of system development that allows recognizing faces from difficult perspectives online and informingtimely about approaching people is undisputed.The manuscript describes the methods of automatic detection of equilibrium face points in the bitmap image andmethods of forming 3D face model. The optimal search algorithm for equilibrium points has been chosen. Themethod of forming 3D face model basing on a single bitmap image and building up the face image rotated to thepreset angle has been proposed. The algorithm for estimating the angle and algorithm of the face image rotationhave been implemented. The manuscript also reviews the existing methods of forming 3D face model. Thealgorithm for the formation of 3D face model from a single bitmap image and a set of individual 3D models havebeen proposed as well as the algorithm for forming different face angles with the calculated 3D face model aimedto create biometric vectors cluster. Operation results of the algorithm for face images formation from differentangles have been presented.\u00a9 2017 Published by Future Academy www.FutureAcademy.org.ukKeywords: Computer vision; face recognition; face position estimation; active shape models; 3D face model; deformable facemodel.", "1. Introduction": "Methods of object recognition are applied to implement software and hardware-software systemsthat provide the human well-being and safety. One of the most complicated objects is a human face ashttp://dx.doi.org/10.15405/epsbs.2017.01.97eISSN: 2357-1330Selection & Peer-review under responsibility of  the Conference Organization Committee736a perfect identification is important to inform about approaching people and to take subsequentdecisions.The task of personal identification based on analysis of the face images is being addressed since theearly stages of the computer vision development (Adini, Moses & Ullman, 1997; Bui, Phan & Spitsyn,2012). Recently the demand for prompt and correct identification of a person in the video stream hasbeen increasing.To date the task of the person search based on the image has been successfully solved and used inmany technical devices. For example, the person search is used by the photo equipment for auto focusselect.The task of the facial recognition (identification) is more complex, and at the moment there is noalgorithm that would be close to the facial recognition rate by the man.In recent years several different approaches to processing, localization and object recognition havebeen proposed. However, these approaches have insufficient accuracy, reliability and speed in the realcontext, which is characterized by the presence of noise in the video sequences and variety of shootingconditions. Methods used to solve the problem of face recognition should provide the acceptablerecognition accuracy and high speed of video sequences processing.Identification efficiency can be estimated on the basis of two probabilistic characteristics:\u2022 probability of false positives (false identification of a person \u2013 a mistaken identity);\u2022 probability of false negatives (omission of a looked-for person \u2013 misrecognition).These probabilities of the identification systems are interrelated variables in the inverse proportion.In addition, all current systems of personal identification have both types of errors being clearlydependent on such factors as shooting angle of the person to be identified, illumination conditions andface image quality both in the database and those recorded with video surveillance cameras.The stringent requirements to the shooting angle of the face to be identified are caused by the usedalgorithms, the best of which do not allow obtaining the resultant satisfactory recognition even at slightturning angle of the facial frontal view towards the axis of the recording camera. Generally, the turningangle is recommended not to exceed 15 degrees.One of possible solutions of the various rotation angles problem is the evaluation and normalizationof the face position in the image. However, there is one more approach to improve recognitionefficiency. Recognition method based on measuring the distance between the clusters of biometricvectors requires that each of the faces in the database must be presented with a set of several differentimages on which the elements of the cluster are computed. This recognition task can be set for one faceimage (a passport photo, etc.), which makes comparison of biometric vectors the inefficient andunreliable method. However, a variety of face images can be produced with using the rotation of thesource on the assumption that the face model is known and corresponds to the image. After suchconversion there is a set of face images with different positions which is suitable for formation ofbiometric vectors cluster.Due to the above the task of face model calculation basing on its image and generation images ofdifferent face angles on the basis of this model becomes relevant.http://dx.doi.org/10.15405/epsbs.2017.01.97eISSN: 2357-1330 / Corresponding Author: A.A. ZakharovaSelection and peer-review under responsibility of the Organizing Committee of the conference7372. Position estimation methods and methods for automatic placement of equilibrium face pointsto the imageMonitoring over human face position against the optical axis of the camera is an important issuedue to the angle-sensitivity of the recognition algorithms. The best-known method of the angleestimation is POSIT (Pose from Orthography and Scaling with Iterations), offering high performanceand rapid convergence (DeMenthon & Davis, 1995).This algorithm is difficult to be applied into practice since it requires the image with the markedcharacteristic points of the face, which is generally not done automatically. The existing methods ofautomatic positioning of the characteristic points are fairly faulty and at small rotation angles it isimpossible to evaluate the face angle with the adequate accuracy.If face characteristic points are properly positioned, the angle estimation algorithm allows obtainingthe coefficients to set the angle correction of the face images and calculating the normalized image.There are several methods for automatic placement of equilibrium face points, all of them are basedon Active Shape Models (ASM) developed by Tim Cootes and Chris Taylor in 1995 (Cootes, Taylor,Cooper & Graham, 1995).This method is widely used to analyze face images, mechanical units and medical images (2D and3D).The authors have investigated various modifications of the original ASM algorithm (Cootes, Taylor,Cooper & Graham, 1995; Milborrow, 2014; Xiong, 2013; King, 2007), and have selected themodification which is more stable in case of larger rotation angles.In general it can be assumed that the accuracy and reliability of points\u2019 alignment methods dependslargely on the method of constructing the training sampling. Fig. 1 shows the comparison of markingequilibrium face points with large angle of rotation relative to the camera lens for several differentASM versions.Fig. 1. a) Marking performed with ASM-modification that trained mainly on the frontal faces; b), c) markings made withoptimally trained ASM-modificationsTest of POSIT algorithm (8-point model, eye corners + mouth) in combination with the automaticalignment of equilibrium face points algorithm showed that the rotation angle estimation error did notexceed 5 degrees on each axis, which was a good result for those given conditions.eISSN: 2357-1330Selection & Peer-review under responsibility of  the Conference Organization Committee7383. Methods of building up 3D face modelThere are different algorithms for the task of face position estimation and automatic placement ofequilibrium face points. At the same time, the task of angle correction has never been considered as amethod of improving the performance of recognition algorithms with bitmap face image. This isprimarily due to the problem of determination of the 3D model of the analyzed face.The issue of obtaining the third missing coordinate can be addressed from different positions:1. The calculation of the image depth from the illumination specific features. This method does notguarantee the accuracy and uniqueness of interpretation due to the inability to take into accountall the possible illumination features in the frame and reflective properties of the surface.However, there are studies on this issue (Zimmermann, 2007).2. Calculation of the image depth from the character the object motion in the frame sequence. Thismethod is suitable only for analysis of images in the video stream, it has specific demands to thequality of face images, and its accuracy depends on the illumination conditions and properties ofthe face detector. This method is used in a number of studies, besides its variation with shootinga stationary object is used in 3D-modeling (Agarwal, 2009).3. Using the previously prepared 3D face model that is aligned with equilibrium points on theimage. This method guarantees the accuracy of the depth estimation, but requires 3D model forthe each face to be processed, as well as the initial fit of the model and image. Moreover,obtaining of the face model for each person requires special equipment. This method is usedmainly in the complex recognition systems with multiple cameras, computing 3D model \"on thefly\", at the moment of the face detection (Shahriar, 2007).Due to the above one can conclude that all mentioned methods for building up 3D model havecertain shortcomings and are difficult to be implemented. Thus a new method for the face modelobtaining needs to be developed.As a basis the method of using the ready-made 3D face model has been chosen, but to simplify thesystem the allowance has been made that it is possible to use some unified model as a depth model forthe face image, assuming a similar relief of the main features, such as eyes, eyebrows, nose and mouth.This allowance will reduce the accuracy of the model, but avoid its calculation for each person.4. Method of face position correctionThe work (Hassner, Harel, Paz & Enbar, 2015) shows the model that is an array of depth values181x122 and it is chosen as a unified 3D face model.The following operations are necessary to solve the problem of face position correction:\u2022 Combining the unified depth model with equilibrium face points obtained by one of thealgorithms described above;\u2022 Splitting of the resulting face model into triangles (triangulation). This operation will allowobtaining a set of triangles describing the face area in the image. It can be used to build anormalized face image, but the result will be fairly rough (Fig. 2).http://dx.doi.org/10.15405/epsbs.2017.01.97eISSN: 2357-1330 / Corresponding Author: A.A. ZakharovaSelection and peer-review under responsibility of the Organizing Committee of the conference739Fig. 2. a) The grid imposed on the original image; b) a normalized image\u2022 Fragmentation of the triangles on the selected criteria that allows reducing the size of the usedtriangles and approximate used triangular grid to the uniform one. As a subdivision criterion onecan take the maximum area of the triangle or the maximum length of the triangle side;\u2022 Each of the triangles is shifted and deformed with rotating operations relative to a given centerand affine transformation in accordance with the angle value determined by POSIT algorithm.As a result of these operations the face image normalized by rotation angles is obtained.5. Possible solutions of angle correction taskThe task of the angle correction may be considered in various ways, and in accordance to the taskbased on 3D face model it is possible to recover the frontal position of the rotated face in the image aswell as to create a set of facial images rotated to different angles relative to the optical axis of thecamera (Fig. 3).Fig. 3. Possible solutions of angles synchronization task for facial recognitionThe majority of face recognition systems for 2D image are based on the principle of clustering ofdifferent images of the same face and subsequent comparison by affinity of the biometric vector of theface image obtained from the video stream to one of the clusters (cluster analysis). There is also avariant of face recognition with pairwise comparison of the standard image and images obtained fromeISSN: 2357-1330Selection & Peer-review under responsibility of  the Conference Organization Committee740video, but the accuracy of this approach is significantly lower, since it is impossible to eliminatecompletely the influence of light, position and shooting quality, so error probability increases.Depending on the choice of the face recognition method for the angle correction one can apply theapproach of bringing all processing faces to the front view (frontalization) as well as the approach thatuses the generation of multiple face images at different angles relative to the optical axis of the camerato form the cluster corresponding to a specific person (generation of face rotations).The best of these options for face recognition in the video stream is the cluster analysis, as it hassuch critical advantages as low computational efforts and high recognition accuracy in various shootingconditions.Regarding the cluster analysis the method of face rotation generation can be used to obtain a set offace images with a single image to build the cluster not degrading the recognition quality.There are three basic approaches to solve the problem of forming 3D model in the situation whereonly a set of bitmap images of the human face is available. The first approach is generation of 3Dmodel from the information received through changing the position of the human head in the video(\"video stream\" in Fig. 3). The second approach uses the unified three-dimensional depth map for thehuman face with specific equilibrium face points in the image (unified model). The third approachmodifies the method of building a unified model with the depth map generated by implicit rules thatobtained by the machine learning of the system based on face images, each of which has thecorresponding 3D model (a deformable model).Each of the methods discussed above has advantages and disadvantages as regards parameters asspeed, accuracy, dependency on the shooting conditions. Below these properties are considered foreach method.1. Video stream:+ Individual face model;-  High computational complexity;-  Low accuracy;-  Strong dependence on recording resolution.2. Unified model:+ The minimum calculation at the stage of model building;+ Minimum dependence on recording conditions;-  Moderate accuracy.3. Deformable model:+ Moderate computational complexity;+ Minimum dependence on recording conditions;+ High accuracy;- Complex model formation.Among methods of forming 3D model the method of the video stream is the least suitable for themethod of cluster analysis, as in non-ideal conditions of shooting it does not have any advantage, butdemands a lot of computational resources. The unified face model is more applicable because of easyhttp://dx.doi.org/10.15405/epsbs.2017.01.97eISSN: 2357-1330 / Corresponding Author: A.A. ZakharovaSelection and peer-review under responsibility of the Organizing Committee of the conference741implementation and minimal resource requirements at all stages of the model building. The deformablemodel has the advantage of the higher model accuracy, but this approach is difficult to implement.6. Algorithm for building up individual face modelsA set of 11 3D face models in the public domain (Paysan, 2009) was taken as a basis for theformation of the deformable face model. Each 3D model contains a cloud of points in three-dimensional space as well as texture information to obtain the original face image in the frontalposition and accordingly to find equilibrium points.If the number of available 3D face models is enough we can take the nearest reference model asrequired (distance between model equilibrium points and equilibrium points of the analyzed face is aminimal on the average), but the more accurate solution is the interpolation of each point of 3D modelby the affinity to the reference models. This method provides a unique model of the analyzed face thatis more accurate than any of the reference models.Due to the above, the following algorithm for the formation of individual facial model has beenproposed:1. Search for the relevance between the found equilibrium points and known 3D models.2. Normalization of equilibrium points relative to models scale (for example, by the distancebetween the eye centers) and the center of coordinates (for example, the nose tip).3. The calculation of distances for each equilibrium point of the face image and the correspondingequilibrium point of each 3D model.4. Calculation of the depth coordinates for equilibrium points of the face image by interpolatingthe values of equilibrium points of the models. The distance between the correspondingequilibrium points of the known model and a preset face image indicates the affinity of 3Dmodels to human face in the image.5. Determination of the depth coordinate for all other points of the desired model with gradualcalculation of new points between known equilibrium points until the number of points in themodel will reach the number of points in reference models.All the above operations will allow obtaining the individual 3D face model, which can be used forthe algorithm of building up the image of the face rotated by preset angle.7. Algorithm testingThe obtained 3D model was used to form face images with different angles for testing the approachvalidity as per following algorithm:1. Combining the obtained face model with face equilibrium points received through one of ASMmethod modifications.2. Splitting the obtained face model into triangles as per equilibrium points (triangulation). Thisoperation will allow obtaining a set of triangles describing the area of the face in the image.Now it can be used for the normalized image building, but the result will be fairly rougheverywhere except equilibrium points.eISSN: 2357-1330Selection & Peer-review under responsibility of  the Conference Organization Committee7423. Fragmentation of the triangles with the selected criterion aimed to reduce the size of the usedtriangles and approximate the built irregular triangular mesh to the uniform one. Thefragmentation criterion could be the maximum area of the triangle or the maximum length of theside. The criterion as per the maximum length provides more proportional mesh.4. Each of the triangles is deformed and replaced in accordance with the predetermined rotationangle by rotating operations relative to the given center and affine transformation.These operations will result in obtaining the face image rotated to a preset angle relative to theoptical axis of the camera.The set of face images rotated by the angle from -20 to +20 degrees relative to the optical axis ofthe camera was obtained from one front image using this algorithm (Fig. 4).Fig. 4. a) The initial frontal facial image; b) the face images obtained by rotating along the horizontal axis by 20 degrees left andrightThese images used for the formation of the biometrical vectors cluster in the person identificationsystem allow using only one photo of the person photo not compromising the recognition efficiency.The above methods of the image processing were combined in different ways. The first testing wascarried out with the use of the video database. The second testing was carried out on the Caltech Facesdatabase (Paysan, 2009). The obtained feature vectors were compared using the Euclidean metric. EERcharacteristic was selected to evaluate the effectiveness of feature vector calculation algorithms. EER isan equal FAR and FRR error rate, where FAR is the probability of false detection, that is theprobability that the system mistakenly recognizes the identity of the user which is not registered in thesystem, and the FRR is the probability of goal missing, that is the probability that the system does notrecognize the identity of the user registered therein. The lower EER is, the more efficient the algorithmis considered (Kukharev, Kamenskaya, Matveev & Schegoleva, 2013).Table 1 shows the results of testing recognition system with the database of 12 front face images,database of 15 images of each face, and database of images created using deformable face models.Table 1. Testing results of face recognition system with various databasesImage database EER, %Single face image for one human 8.0686715 face images for one human 4.8198615 face images generated by the algorithm 2.17865http://dx.doi.org/10.15405/epsbs.2017.01.97eISSN: 2357-1330 / Corresponding Author: A.A. ZakharovaSelection and peer-review under responsibility of the Organizing Committee of the conference743Basing the results shown in Table 1 we can conclude that the method of generating face images withdifferent angles does not reduce the efficiency of person recognition and in some cases improves therecognition result."}{"id": "80133284", "Abstract": "Contemporary knowledge is transforming to information and takes specific features in order to participate in a constantdevelopment, moving and converting to informational floods. I argue that in the new informational epistemology the relationbetween subject and object are changing. The character of Knowledge is interdisciplinary. The consequence of such tendency inmodern science is accent varying from result of scientific discovery to the process. Now it is important to know how, in whatway scientists extract scientific result. Knowledge is break up into informational flows, which \u201cget tangled\u201d, \u201cflash\u201d and\u201cpulsate\u201d.The cognitive management is presented in this paper as a tool for knowledge use in constantly changing informational floods.Administrative mechanism of knowledge management made accent on placing emphasis on procedural learning tools, adaptsprocess of mastering by knowledge for conditions and requirements of a modern information age. New character of cognitiveprocesses is caused by the new informative means which have appeared together with the Internet, e-mail and system of masscommunication. They connected the world in uniform space.\u00a9 2014. The Authors. Published by Elsevier Ltd.Selection and peer-review under responsibility of Tomsk Polytechnic UniversityKeywords: knowledge management, information society, post-industrial society, cognitive science.", "1. Introduction": "The distinctive feature of modern civilization is forming of new society type called \u2013 \u201cInformation society\u201d. Atpresent there isn\u2019t universally concept accepted of what exactly can be termed information society. The Japaneseversion of term \u201cinformation society\u201d (johoshakai, johokashakai) appeared during a conversation KishoKurokawaand Tudao Umesaoin 1961. (Karvalics, 2007, p.5)The theory of information society was presented by Porat, Rubin, Masuda, Stonier, Katz. (See Porat,, Rubin, 1987;Masuda, 1981).According to Toffler and Naisbitt, the information society in the United States appeared in the end of 1950s. Theproblem of defining the notion \u201cinformation society\u201d consists in different basis distinguished by different authors.There are dimensions of the information society according to Daniel Bell (Bell, 1976, 1980): economics,technology, skill base, strategic recourse, methodology, design, and actual principal. Yoneji Masuda in his*Pogukaeva Nataliya Tel.:+7-913-825-88-53\u2026E-mail address: pogukaeva@mail.ru5 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license(http://creativecommons.org/licenses/by-nc-nd/3.0/).P er-review under responsibility of Tomsk Pol technic University.457 Pogukaeva Nataliya /  Procedia - Social and Behavioral Sciences  166 ( 2015 )  456 \u2013 459comparison of the characteristics of industrial and information society conclude that information society haveadvanced technology instead of useful goods and services, increase in knowledge frontiers, information space,information industry, knowledge industry, high mass knowledge creation, time-value (Masuda, 1980).Daniel Bell formulated the theory of post-industrial society postulate information as indicator for the informationalcharacter of a society.Jean-Francois Lyotard has argued that \u201cknowledge has become a principle \u2026 force of production over the last fewdecades. Knowledge would be transformed into a commodity.\u201d (Lyotard, 1984).All designated measurements contemporary society departs from increasing knowledge role which uninterruptedlytransforming to information. Toffler argues that knowledge is the central resource in the economy of the informationsociety: \u201cIn a Third Wave economy, the central resource \u2013 a single word broadly encompassing data, information,images, symbols, culture, ideology and values \u2013 is considered actionable knowledge\u201d (Toffler, 2004. p.154)In such approach we could use the following terms: \u00abthe knowledgeable society\u00bb, \u00abknowledge society\u00bb or\u00abknowledge-value society\u00bb. So, we can assume that knowledge and information are in dialectical tie. It is possibleto make such contradictory assumption because knowledge and information are quite ambiguous from the start.According to Laszlo Karvalics and common sense in the 20th century the most developed countries graduallyentered the state of information society and it is expected that within a matter of a few decades the majority of theworld\u2019s population will be living and working in a global information society. (Karvalics, 2007.p. 21)Cognitive management is the one adequate tool for using knowledge in society where information plays a keyrole. It is a part of business strategy, information technology and human resource management for many largecompanies.2. Knowledge societyI want to emphasize that knowledge in information society very \u201cflexible\u201d. So for today is not possible to imaginethe \u201cconstant\u201d knowledge, like it was in Aristotle\u2019s epoch, when truth was unchangeable for a long period of time.Initially, fundamental science is developing in new conditions, where information is not under the control.Human consciousness is sink in information, this point allow speaking about totally new type of mentationdifferent from classical rationality.Modern culture does not have sociocultural dominating idea \u2013 arhe, like previous epochs (antiquity \u2013cosmocentrism, middle ages \u2013 Theo centrism, Renaissance \u2013 pantheism, Enlightenment. We can consider that todayknowledge play this role.Today there are no any algorithms, chaos, and constant crisis \u2013 knowledge is creating in the surviving.In infinity movement the knowledge loses a classical approach to the subject (knowledge character isinterdisciplinary). Informational flows in close interacting messaging to knowledge interdisciplinary characteristic.Thus, contemporary science does not have strict dividing of subjects as classical science.Innovating truth today appeared in the intersection\u2019s field of different sciences which could not be happen early.Knowledge management is presenting this new function of modern science. It is developing in new conditionswhere algorithmic purity of possible situations is lost.Modern epistemology is paying attention on cognitive process, on skills and abilities to detect truth moving.Today informational conditions are not dictate constant combined professional of cultural skills. Main skills rapidlyare able to orientate changing world. Information is not under control, it is free from control of pure reason. If weneed the instrument for knowledge management, then cognitive management could play this role.New ideas of management are generate by cognitive revolution of humanitarian knowledge, which took place inXX century. We could assert that cognitive term have place in all human and society sciences. There aresuccessfully developing cognitive sociology, cognitive linguistic, cognitive politology.Finally, cognitology itself integrates knowledge in different subjects and united linguists, mathematics, andphilosophers. Cognitive term is in the first place an attempt to understand experience, subjectiveness and actions ofindividual. Also to penetrate into essence interpersonal relationships, collective activity, social institutes from aposition of distribution and use of knowledge. Such context is highlighting cognitive process leaving other types ofindividual and collective activity beyond limits.458   Pogukaeva Nataliya /  Procedia - Social and Behavioral Sciences  166 ( 2015 )  456 \u2013 459Scientific cognitive term and higher attention to the cognitive process are answer on informational shape ofknowledge which is aloof both from the subject of scientific knowledge, and from educational subjectivity.2. 2. Knowledge is source of effectivenessModern information society needs from individuals to be flexible to knowledge. There is a lot of knowledgemanagement programs can give remarkable advantage to individuals and organizations if they are intentional andaction oriented.Knowledge is not only the independent value, but occurs multiplicative effect toward other production factors  andinfluence on the level of effectiveness. Thus, the contemporary economics use knowledge as a resource ofsuccessful competition instead of advantageous market position. The attention centre is movement andknowledge management not knowledge creation.An actual research in knowledge management is presented by academic Ikujiro Nonaka (Hitotsubashi University).In 2008, the Wall Street Journal listed him as one of the most influential persons on business thinking, and TheEconomist included him in its \"Guide to Management Ideas and Gurus\". Nonaka has proposed the SECI model(Socialization, Externalization, Combination, and Internalization) one of the most widely cited theories inknowledge management (Gourlay, 2003. p.18). There is a shape of knowledge distinguishes between tacitknowledge and explicit knowledge (Nonaka, 1991 p.97).In dependence on direction of informational flows knowledge management must provide with control forrealization spiral SECI model in company. Main task for managers is to be responsible for effective function of thisspiral model. For this reason Nonaka and Takeuchi use the model organization concept, where way of managementlook like \u201cfrom the centre \u2013 up \u2013 down\u201d, where managers of middle team in a centre of actions.They are conductorsof ideas between turn off from reality and putting forward time idealistic concepts for top managers and ordinary,routine activity for ordinary employees which these concepts have to realize: creation of new knowledge; use ofavailable knowledge at decision-making; embodiment of knowledge in products and services; transfer of existingknowledge from one part of the organization into another; ensuring access to necessary knowledge, and alsoprotection of knowledge.3. Methodological base of knowledge managementKnowledge in a shape of information considerably differs from its classical understanding. Informational societyis essential transformation of many concepts. Particularly classical cogito discovered by Descartes was understoodas seek of truth. The truth and objective world was in opposition to each other. Classical epistemology isinterpreting this as opposition of active subject and passive, stable object. It follows that pure knowledge is stable,objective, perpetual and absolutely. Modern knowledge is developing in unstable, virtual world. It is world ofinformation with computer intellectual system shape. There is a main effect of internet and virtual reality thatsociety fills up with different \u201cforms of reality\u201c and individuals must keep it in their minds.In contemporary computer epistemology, subject is lost traditional relationships with real objective world. Itmeans that object could not be subordinate to omnipotent cogito. Now the object so active, changeable, it makessubject always be ready to change. The subject having been connected with a set of realities mast develops to itselfabilities of abstraction, imagination, creativity and fast decision-making. In such situation there are importantabilities such as learning and memorizing, which are pushing to the background. Encyclopedic knowledge issurrendering the initiative to developing knowledge. In this case knowledge management is management ofcognitive process.In classical epistemology of R. Descartes and I. Kant the world is represented as stable, the same and constant.The truth extracted from such world could be forever exact. That character of cognition make transcendentalguaranty of classical philosophy with characteristics of consistency, UN ambiguity and strictness. On the contrary,knowledge in a shape of information is extremely movable. Informational knowledge constantly nascent as new isdemand reconsideration in new contexts.To retract itself movement with inner potential to constant development modern knowledge is in a condition ofpermanentreadiness to change. This movable condition of information is provoking its innovation.459 Pogukaeva Nataliya /  Procedia - Social and Behavioral Sciences  166 ( 2015 )  456 \u2013 459Distinguishing characteristic of information is being in isolation with subject. \u201cSeparating knowledge from subjectit is information\u201d (Petrova, 2013 p. 95). Thereby knowledge and information there are different notions towardssubject.Such independence of innovation knowledge needs a new management mechanism. Knowledge managementaddressing to procedural innovation applies to be this mechanism. The aim of knowledge management is to adaptthought to resonanceworking conditions. It could promote management origin at information, which now free fromsubject control. Cognitive processes, strategy of knowledge management is directed on administration with which,aren't absolutely new in comparison with those to processes which characterized the person always.However the content and character of cognitive processes had change. The necessity of knowledge management isconnected with such kind of changing."}